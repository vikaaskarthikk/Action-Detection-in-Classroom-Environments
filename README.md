# ğŸ¯ Action Detection in Classroom Environment  
A real-time deep-learning system for detecting student actions such as **sitting, standing, walking, writing, sleeping, raising hand, and phone usage** using **YOLOv5**, **FastAPI**, and **React.js**. The project enables automated classroom monitoring, behavior analytics, and PDF report generation.

---

## ğŸ“Œ Project Overview
This project implements a complete end-to-end pipeline to detect student behaviors from classroom videos using **YOLOv5 object detection**. It processes input videos, identifies actions frame-by-frame, and provides a detailed visual dashboard along with downloadable PDF reports.

The system achieves:

- âœ… **92.3% accuracy**  
- âš¡ **18 FPS real-time performance**  
- ğŸ“Š **Action analysis & proximity detection**  

It is designed for smart classroom environments to improve engagement analysis, discipline tracking, and automated monitoring.

---

## â­ Key Features
- ğŸ” Real-time action detection using YOLOv5  
- ğŸ¥ Video upload + automatic processing  
- ğŸ“Š Action distribution & behavior analytics  
- ğŸ“ PDF report generation (jsPDF)  
- ğŸŒ Modern React UI with Tailwind CSS  
- âš¡ FastAPI backend for high-speed inference  
- ğŸ” Privacy-focused (no data stored, edge processing)  
- ğŸ“ Proximity-based action mapping  

---

## ğŸ§± System Architecture
```
React Frontend  â†’  FastAPI Backend  â†’  YOLOv5 Model  â†’  Results + PDF Report
```

### Components:
- **Frontend:** React.js, TypeScript, Tailwind CSS  
- **Backend:** FastAPI, Python, PyTorch, OpenCV  
- **Model:** YOLOv5 (custom-trained)  
- **Reporting:** jsPDF + AutoTable  

---

## ğŸ—‚ï¸ Dataset & Annotation
Dataset includes actions:

- Sitting  
- Standing  
- Walking  
- Using Phone  
- Writing  
- Sleeping  
- Raising Hand  

Annotated using **CVAT / Roboflow**.

Dataset structure:

```
dataset/
â”‚â”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚â”€â”€ labels/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚â”€â”€ data.yaml
```

---

## ğŸ¤– Model Training (YOLOv5)

### Training Example (YOLOv5)

```bash
python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --weights yolov5s.pt
```

### Resuming Training
```bash
python train.py --resume
```

### Inference Example
```bash
python detect.py --weights best.pt --source video.mp4 --save-txt --save-conf
```

### Model Performance  
- **Accuracy:** 92.3%  
- **Inference Speed:** 18 FPS  
- **Precision/Recall:** High, stable  
- **Confusion Matrix:** Strong class-wise accuracy  
- **mAP:** As per validation metrics in training logs  

---

## ğŸ”Œ Backend (FastAPI)
Backend handles:

- Uploading video  
- Running YOLOv5 inference  
- Frame-by-frame analytics  
- Sending JSON results to frontend  
- Logging timestamps & counts  

### Run Backend
```bash
uvicorn app:app --reload
```

---

## ğŸ’» Frontend (React)
Frontend features:

- Drag-and-drop video upload  
- Select **Action Detection** or **Proximity Mode**  
- Real-time playback of annotated video  
- Statistical dashboard for each processed video  
- Generate PDF report  

### Run Frontend
```bash
npm install
npm start
```

---

## ğŸ“ PDF Report Generation
Reports include:

- Total frames, FPS, duration  
- Action counts & timestamps  
- Graphs and summary tables  
- Keyframes from detected actions  
- Proximity analysis insights  

Generated using:

- **jsPDF**
- **jsPDF AutoTable**

---

## ğŸ§ª Results Summary

### Processing Metrics
- **Total Frames:** 703  
- **Video Duration:** 24.24 seconds  
- **Processing Speed:** 15.55 FPS  
- **Report Mode:** Action / Proximity  

### Detection Distribution
- **Sitting:** 50.8%  
- **Bench:** 30.2%  
- **Standing:** 19.0%  

### Model Performance Graphs
- Loss curve (box loss, cls loss, dfl loss)  
- F1 curve  
- Precision-confidence curve  
- Precision-recall curve  
- Confusion matrix  

---

## ğŸ“ Project Structure
```
Action-Detection-Classroom/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ YOLOv5_training.ipynb
â”‚
â”‚â”€â”€ reports/
â”‚
â”‚â”€â”€ README.md
```

---

## ğŸ”® Future Enhancements
- Transformer-based action recognition  
- Multi-modal fusion (audio + video)  
- Real-time deployment on GPU servers  
- On-device (mobile/edge) inference  
- Multi-camera classroom monitoring  
- LMS Integration for learning analytics  

---

## ğŸ‘¨â€ğŸ’» Contributor
**Vikaas Karthik K â€“ 1RV23SCN17**  
Department of CSE  
RV College of Engineering (RVCE)

- GitHub: https://github.com/vikaaskarthikk  
- Email: vikaaskarthik.k@gmail.com  

---

## ğŸ“œ License
Licensed under the **MIT License**.
